{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepL_FinalProject_Colab_Iteration04012020_Airport.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1VDRp2gxkgk"
      },
      "source": [
        "### **Toulouse School of Economics**\n",
        "#### **M2 Statistics & Econometrics**\n",
        "---\n",
        "\n",
        "### **Mathematics of Deep Learning Algorithms, Part 2**\n",
        "# **Final Project: *Performance Benchmarking of Different Information Retrieval Methods***\n",
        "\n",
        "### **Anh-Dung LE, Paul MELKI**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL_0ZCqPxkgr"
      },
      "source": [
        "In this project, we aim at comparing the performance of different Information Retrieval techniques, mainly: **BM25** and **BERT-based search engine**. We work on a corpus formed of the latest dump of English Wikipedia, and restrict our work to only a small subset of this dump (mainly, articles whose title starts with the letter 'A'), and that is due to unavailability of enough computational resources. \n",
        "\n",
        "But first, we start with some preliminary steps: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnlAZckrxkgs"
      },
      "source": [
        "### **Preliminaries & Corpus Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ9X7zzqygvA",
        "outputId": "da34f9ed-94e3-49d3-c213-75e04ffe6554"
      },
      "source": [
        "# Install required libraries\r\n",
        "!pip install rank-bm25"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank-bm25) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIAgpAGhIPSc",
        "outputId": "8a0391a2-2e4e-4729-ce4a-370ad936313d"
      },
      "source": [
        "pip install -q tf-models-official==2.3.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 849kB 4.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 9.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 15.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 1.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 48.8MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CioRA6p1yv2F",
        "outputId": "092f8ce1-a2ad-44e8-b8bf-3867386e68d6"
      },
      "source": [
        "# Define the path to the project's directory\r\n",
        "PATH = '/content/drive/MyDrive/College Material/Master 2/Mathematics of Deep Learning Algorithms/Final Project'\r\n",
        "\r\n",
        "# Load drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRLmf8D_xkgs"
      },
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import pprint as pp\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf \n",
        "from gensim.corpora import WikiCorpus\n",
        "from rank_bm25 import BM25Okapi, BM25Plus\n",
        "\n",
        "from official.modeling import tf_utils\n",
        "from official import nlp\n",
        "from official.nlp import bert\n",
        "\n",
        "# Load the required submodules\n",
        "import official.nlp.optimization\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.run_classifier\n",
        "import official.nlp.bert.tokenization\n",
        "import official.nlp.data.classifier_data_lib\n",
        "import official.nlp.modeling.losses\n",
        "import official.nlp.modeling.models\n",
        "import official.nlp.modeling.networks"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL3Mu_iExkgt"
      },
      "source": [
        "In order to create our own local textual corpus based from Wikipedia, we make use of the class `WikiCorpus` implement in the `gensim.corpora` library. This class implements different functions that facilitate the handling and manipulation of Wikipedia dumps, which are usually downloaded as BZ2-compressed XML files.\n",
        "\n",
        "Based on this library, we create our own function to read and save the corpus locally, with each Wikipedia being saved in its own `.txt` file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBoh4qKjxkgt"
      },
      "source": [
        "# Define function to read and create corpus from downloaded dump\n",
        "def make_corpus(in_file, out_directory):\n",
        "    \"\"\"\n",
        "    Function that converts a Wikipedia .xml dump into a \n",
        "    corpus, saving each article in a separate .txt file.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    @param in_file: str, \n",
        "        A valid string specifying the path to the local *.xml.bz2 Wikipedia \n",
        "        dump file.\n",
        "    @param out_directory, str,\n",
        "        A valid string specifying the path to the directory in which we wish to\n",
        "        save the created .txt files.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Instantiate WikiCorpus object, based on the local dump file.\n",
        "    wiki = WikiCorpus(in_file)\n",
        "    print(\"Corpus is read!\")\n",
        "    \n",
        "    # Initialize counter of articles read.\n",
        "    i = 0\n",
        "    \n",
        "    print(\"Getting texts...\")\n",
        "    # For new article read, do...\n",
        "    for text in wiki.get_texts():\n",
        "        # Create and open new file for new article.\n",
        "        output_file = open(f'{out_directory}\\\\{str(i+1)}.txt', 'w')\n",
        "        # Extract the text of the read article.\n",
        "        article_text = bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n'\n",
        "        # Take only first 1000 words from each article, to keep sizes small.\n",
        "        first_n_words = ' '.join(article_text.split(' ')[0:1000])\n",
        "        # Write text to file & close the file.\n",
        "        output_file.write(first_n_words)\n",
        "        output_file.close()\n",
        "        # Update counter\n",
        "        i = i + 1\n",
        "        # If 1000 articles have been read, stop reading.\n",
        "        if (i % 1000 == 0):\n",
        "            print(f'Processed {str(i)} articles')\n",
        "            break\n",
        "\n",
        "    print('Processing Complete!')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HWHc4OzFxkgu",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "146fb6d2-bb01-4c8d-bd2d-a62712f2dfcb"
      },
      "source": [
        "# Initialize input and output paths\n",
        "in_path = \"C:\\\\Users\\\\Paul\\\\Documents\\\\Python Scripts\\\\Data\\\\enwiki-latest-pages-articles1.xml-p1p41242.bz2\"\n",
        "out_path = \"C:\\\\Users\\\\Paul\\\\Documents\\\\Python Scripts\\\\Data\\\\Wiki Corpus\"\n",
        "\n",
        "# Create corpus!\n",
        "make_corpus(in_path, out_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'make_corpus' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-3-24fb57d3dadc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create corpus!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmake_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'make_corpus' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oirCM0kxkgv"
      },
      "source": [
        "Now that the corpus is created, we also need to create a function to read the corpus from the files we created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y1fthbUxkgv"
      },
      "source": [
        "def read_corpus(corpus_directory):\n",
        "    \"\"\"\n",
        "    Function that iteratively reads the saved articles from the corpus directory\n",
        "    and appends the text to a list.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    @param corpus_directory: str,\n",
        "        A valid string specifying the path to the local directory in which the \n",
        "        files were saved using make_corpus().\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    @return corpus, list\n",
        "        A list containing the text of an article in each element.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize empty corpus list\n",
        "    corpus = []\n",
        "    \n",
        "    # For each file in the corpus directory, do...\n",
        "    print(\"Reading local corpus, please wait...\")\n",
        "\n",
        "    for filename in os.listdir(corpus_directory):\n",
        "        file = open(f'{corpus_directory}/{filename}', 'r',\n",
        "                    encoding=\"utf8\")\n",
        "        article_text = file.read()\n",
        "        corpus.append(article_text)\n",
        "        \n",
        "    # Done, return\n",
        "    print(\"Done!\")\n",
        "    return corpus"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "EJaX5viHxkgv",
        "outputId": "803e684f-a7b9-4b4d-d561-c97a932547b7"
      },
      "source": [
        "# Read corpus! \n",
        "corpus = read_corpus(f'{PATH}/Wiki Corpus/')\n",
        "\n",
        "# Look at some example...\n",
        "corpus[3][0:100]\n",
        "\n",
        "# For some reason (only in Google Colab), this cell might need to be stopped\n",
        "# for the first run, then run again. "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading local corpus, please wait...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'anarchism is political philosophy and movement that is sceptical of authority and rejects all involu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_vMqfPMxkgw"
      },
      "source": [
        "### **BM25 Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqMNon0Vxkgw"
      },
      "source": [
        "The first Information Retrieval method we try is the **BM25** method, which is a TF-IDF method, that retrieves the article that has the highest score based on the query given. \n",
        "\n",
        "Given, a document $D$ and a $Q$ that contains keywords $q_1,..., q_n$, we define the BM25 score of the document $D$ as:\n",
        "\n",
        "$$\n",
        "score(D, Q) = \\sum_{i = 1}^n IDF(q_i) \\cdot \\frac{TF(q_i, D) \\cdot (k_1 + 1)}{TF(q_i, D) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{|D|}{avgdl} \\right)}\n",
        "$$\n",
        "\n",
        "where: \n",
        "- $TF(q_i, D)$ is the *text frequency* of keyword $q_i$ in document $D$,\n",
        "- $IDF(q_i)$ is the *inverse document frequency* of keyword $q_i$, using the well-known definition,\n",
        "- $|D|$ is the length of the document $D$ in words.\n",
        "- $avgdl$ is the average document length in words in the whole corpus.\n",
        "- $k_1$ and $b$ are free parameters that are chosen rather than estimated, and which are usually chosen as $k_1 \\in [1.2, 2.0]$ and $b = 0.75$. These may also be chosen based on some advanced optimization.\n",
        "\n",
        "After computing the BM25 score of each document, which gives the relevance of each document to the given query, we sort the documents in descending order from most relevant to least relevant.\n",
        "\n",
        "On the implementation side, we use `Rank-BM25` library developed by Dorian Brown (https://github.com/dorianbrown/rank_bm25), and which implements different variants of the BM25 algorithm. It can be easily installed using `pip install rank-bm25`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH8byUmmxkgw"
      },
      "source": [
        "# Tokenize the corpus\n",
        "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
        "\n",
        "# Instantiate BM25 object from the tokenized corpus\n",
        "bm25 = BM25Okapi(tokenized_corpus)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfumgpPIxkgx"
      },
      "source": [
        "Now we create  a simple function that a takes a string query, and a number `n` of required results, and returns the `n` most relevant results from our corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doFTbRAcxkgx"
      },
      "source": [
        "def bm25okapi_search(tokenized_query, bm25, corpus, n_results = 1):\n",
        "    \"\"\"\n",
        "    Function that takes a tokenized query and prints the first 100 words of the \n",
        "    n_results most relevant results found in the corpus, based on the BM25\n",
        "    method.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    @param tokenized_query: list, array-like\n",
        "        A valid list containing the tokenized query.\n",
        "    @param bm25: BM25 object,\n",
        "        A valid object of type BM25 (BM25Okapi or BM25Plus) from the library\n",
        "        `rank-bm25`, initialized with a valid corpus.\n",
        "    @param corpus: list, array-like\n",
        "        A valid list containing the corpus from which the BM25 object has been \n",
        "        initialized. As returned from function read_corpus().\n",
        "    @param n_results: int, default = 1\n",
        "        The number of top results to print.\n",
        "    \"\"\"\n",
        "    \n",
        "    # We skip checking validity of arguments for now... We assume the user \n",
        "    # knows what they're doing.\n",
        "    \n",
        "    # Get top results for the query\n",
        "    top_results = bm25.get_top_n(tokenized_query, corpus, n = n_results)\n",
        "    \n",
        "    # Take only first 100 words from each result\n",
        "    top_results_100words = [' '.join(top_result.split(' ')[0:100]) \n",
        "                             for top_result in top_results]\n",
        "    \n",
        "    # Print results\n",
        "    print(f'Query: \"{query}\"\\n')\n",
        "    print(f'Top {n_results} results from Wikipedia:\\n\\n')\n",
        "    i = 1\n",
        "    for result in top_results_100words: \n",
        "        print(f'{i}. {result}\\n\\n')\n",
        "        i = i + 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkbL85nhxkgx"
      },
      "source": [
        "As we know the topics of some of the articles included, we implement some queries about these topics and see whether their relevant articles are returned. Some of these topics included:\n",
        "- Autism\n",
        "- Anarchism \n",
        "- ATM \n",
        "\n",
        "We first try to implement some simple queries that include only the title of the article, and see if the relevant article is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP5cZkp_xkgy",
        "outputId": "03a687cd-43d7-4090-dea4-015eec7e97f0"
      },
      "source": [
        "query = \"autism\"\n",
        "tokenized_query = query.split(\" \")\n",
        "bm25okapi_search(tokenized_query = tokenized_query,\n",
        "                 bm25 = bm25, \n",
        "                 corpus = corpus,\n",
        "                 n_results = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: \"autism\"\n",
            "\n",
            "Top 5 results from Wikipedia:\n",
            "\n",
            "\n",
            "1. autism is developmental disorder characterized by difficulties with social interaction and communication and by restricted and repetitive behavior parents often notice signs during the first three years of their child life these signs often develop gradually though some children with autism experience worsening in their communication and social skills after reaching developmental milestones at normal pace autism is associated with combination of genetic and environmental factors risk factors during pregnancy include certain infections such as rubella toxins including valproic acid alcohol cocaine pesticides lead and air pollution fetal growth restriction and autoimmune diseases controversies surround other proposed environmental causes for\n",
            "\n",
            "\n",
            "2. alfonso cuarón born november is mexican film director screenwriter producer cinematographer and editor his other notable films from variety of film genres including the family drama little princess the romantic drama great expectations the coming of age road comedy film tu mamá también the fantasy film harry potter and the prisoner of azkaban and the science fiction dystopian thriller children of men several of his films have received critical acclaim and accolades he has been nominated for academy awards winning four of them including two best director awards for gravity and roma he is the first latin american director to\n",
            "\n",
            "\n",
            "3. asa as an abbreviation or initialism may refer to biology and medicine accessible surface area of biomolecule accessible to solvent acetylsalicylic acid aspirin advanced surface ablation refractive eye surgery anterior spinal artery the blood vessel which supplies the anterior portion of the spinal cord aciduria disorder of the urea cycle asa physical status classification system rating of patients undergoing anesthesia education african studies association of the united kingdom african studies association albany students association at massey university auckland new zealand alexander smith academy in houston texas alpha sigma alpha national sorority american society for aesthetics american student assistance american studies\n",
            "\n",
            "\n",
            "4. the americans with disabilities act of or ada is civil rights law that prohibits discrimination based on disability it affords similar protections against discrimination to americans with disabilities as the civil rights act of which made discrimination based on race religion sex national origin and other characteristics illegal and later sexual orientation in addition unlike the civil rights act the ada also requires covered employers to provide reasonable accommodations to employees with disabilities and imposes accessibility requirements on public accommodations in the national council on disability had recommended the enactment of an americans with disabilities act ada and drafted the\n",
            "\n",
            "\n",
            "5. events the battle of hormozdgan is fought ardashir defeats and kills artabanus effectively ending the parthian empire emperor constantius ii enters rome for the first time to celebrate his victory over magnus magnentius assassination of conrad of montferrat conrad king of jerusalem in tyre two days after his title to the throne is confirmed by election the killing is carried out by hashshashin nichiren japanese buddhist monk propounds namu myōhō renge kyō for the very first time and declares it to be the essence of buddhism in effect founding nichiren buddhism the battle of cerignola is fought it is noted\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbFX0kUoxkgy",
        "outputId": "3a2f7081-7d9e-49a7-e451-789736d97714"
      },
      "source": [
        "query = \"anarchism\"\n",
        "tokenized_query = query.split(\" \")\n",
        "bm25okapi_search(tokenized_query = tokenized_query,\n",
        "                 bm25 = bm25, \n",
        "                 corpus = corpus,\n",
        "                 n_results = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: \"anarchism\"\n",
            "\n",
            "Top 5 results from Wikipedia:\n",
            "\n",
            "\n",
            "1. anarchism is political philosophy and movement that is sceptical of authority and rejects all involuntary coercive forms of hierarchy anarchism calls for the abolition of the state which it holds to be undesirable unnecessary and harmful it is usually described alongside libertarian marxism as the libertarian wing libertarian socialism of the socialist movement and as having historical association with anti capitalism and socialism the history of anarchism goes back to prehistory when humans arguably lived in anarchistic societies long before the establishment of formal states realms or empires with the rise of organised hierarchical bodies scepticism toward authority also rose\n",
            "\n",
            "\n",
            "2. anarcho capitalism is political philosophy and economic theory that advocates the elimination of centralized states in favor of system of private property enforced by private agencies free markets and the right libertarian interpretation of self ownership which extends the concept to include control of private property as part of the self in the absence of statute anarcho capitalists hold that society tends to contractually self regulate and civilize through participation in the free market which they describe as voluntary society anarcho capitalists support wage labour and believe that neither protection of person and property nor victim compensation requires state in\n",
            "\n",
            "\n",
            "3. ayn rand born alisa zinovyevna rosenbaum march was russian american writer and philosopher she is known for her two best selling novels the fountainhead and atlas shrugged and for developing philosophical system she named objectivism born and educated in russia she moved to the united states in she had play produced on broadway in and after two early novels that were initially unsuccessful she achieved fame with her novel the fountainhead in rand published her best known work the novel atlas shrugged afterward she turned to non fiction to promote her philosophy publishing her own periodicals and releasing several collections\n",
            "\n",
            "\n",
            "4. events the battle of hormozdgan is fought ardashir defeats and kills artabanus effectively ending the parthian empire emperor constantius ii enters rome for the first time to celebrate his victory over magnus magnentius assassination of conrad of montferrat conrad king of jerusalem in tyre two days after his title to the throne is confirmed by election the killing is carried out by hashshashin nichiren japanese buddhist monk propounds namu myōhō renge kyō for the very first time and declares it to be the essence of buddhism in effect founding nichiren buddhism the battle of cerignola is fought it is noted\n",
            "\n",
            "\n",
            "5. events pope eusebius is banished by the emperor maxentius to sicily where he dies perhaps from hunger strike pope leo ii begins his pontificate byzantine bulgarian wars battle of the gates of trajan the bulgarians under the comitopuli samuel and aron defeat the byzantine forces at the gate of trajan with byzantine emperor basil ii barely escaping georgenberg pact ottokar iv duke of styria and leopold duke of austria sign heritage agreement in which ottokar gives his duchy to leopold and to his son frederick under the stipulation that austria and styria would henceforth remain undivided karl topia the ruler\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR01whIsWK1X"
      },
      "source": [
        "In the above two queries, we see that the results obtained are relevant. For the query about \"autism\", only the top result seems to be relevant. However, this could be simply due to the unavailability of more relevant articles in the small corpus we have, and not due to a problem in the method.\r\n",
        "\r\n",
        "In the second query related to \"anarchism\", we see that the top three results are relevant indeed: the first one being an article exactly related to the topic, the second one being a related one and the third being about an author (Ayn Rand) who wrote many pieces and books about anarchism.\r\n",
        "\r\n",
        "So far, BM25 looks like a usesful method. However, we will see how it fails when the queries become more complicated, such as when they contain a question, a whole sentence, or an abbreviation. We search for an abbreviation (\"ATM\") and look at the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YdUabwRxkgy",
        "outputId": "909668c9-a3a0-494f-c1c4-73bf0e72e202"
      },
      "source": [
        "query = \"ATM\"\n",
        "tokenized_query = query.split(\" \")\n",
        "bm25okapi_search(tokenized_query = tokenized_query,\n",
        "                 bm25 = bm25, \n",
        "                 corpus = corpus,\n",
        "                 n_results = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: \"ATM\"\n",
            "\n",
            "Top 5 results from Wikipedia:\n",
            "\n",
            "\n",
            "1. events the battle of hormozdgan is fought ardashir defeats and kills artabanus effectively ending the parthian empire emperor constantius ii enters rome for the first time to celebrate his victory over magnus magnentius assassination of conrad of montferrat conrad king of jerusalem in tyre two days after his title to the throne is confirmed by election the killing is carried out by hashshashin nichiren japanese buddhist monk propounds namu myōhō renge kyō for the very first time and declares it to be the essence of buddhism in effect founding nichiren buddhism the battle of cerignola is fought it is noted\n",
            "\n",
            "\n",
            "2. an abscess is collection of pus that has built up within the tissue of the body signs and symptoms of abscesses include redness pain warmth and swelling the swelling may feel fluid filled when pressed the area of redness often extends beyond the swelling carbuncles and boils are types of abscess that often involve hair follicles with carbuncles being larger they are usually caused by bacterial infection often many different types of bacteria are involved in single infection in the united states and many other areas of the world the most common bacteria present is methicillin resistant staphylococcus aureus rarely\n",
            "\n",
            "\n",
            "3. the alan parsons project were british rock band active between and whose core membership consisted of alan parsons and eric woolfson they were accompanied by varying number of session musicians and some relatively consistent session players such as guitarist ian bairnson arranger andrew powell bassist and vocalist david paton drummer stuart elliott and vocalists lenny zakatek and chris rainbow parsons was an audio engineer and producer by profession but also musician and composer songwriter by profession woolfson was also composer pianist and singer almost all the songs on the project albums are credited to woolfson parsons the alan parsons project\n",
            "\n",
            "\n",
            "4. the atanasoff berry computer abc was the first automatic electronic digital computer limited by the technology of the day and execution the device has remained somewhat obscure the abc priority is debated among historians of computer technology because it was neither programmable nor turing complete the first real programmable and turing complete machines the and the colossus computer from used similar valve based technology as abc overview conceived in the machine was built by iowa state college mathematics and physics professor john vincent atanasoff with the help of graduate student clifford berry it was designed only to solve systems of\n",
            "\n",
            "\n",
            "5. peas are an annual plant an annual plant is plant that completes its life cycle from germination to the production of seeds within one growing season and then dies the length of growing seasons and period in which they take place vary according to geographical location and may not correspond to the four traditional seasonal divisions of the year with respect to the traditional seasons annual plants are generally categorized into summer annuals and winter annuals summer annuals germinate during spring or early summer and mature by autumn of the same year winter annuals germinate during the autumn and mature\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im_EvjZrWQf2"
      },
      "source": [
        "We can clearly see that the results obtained are not relevant to queried term.\r\n",
        "\r\n",
        "Let's try to query for the same topics as above, but using a more complicated query:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zbDxDaZWWMR",
        "outputId": "ffccb5cf-ef30-4f63-e13a-3f3cc9d136d9"
      },
      "source": [
        "query = \"what is anarchism?\"\r\n",
        "tokenized_query = query.split(\" \")\r\n",
        "bm25okapi_search(tokenized_query = tokenized_query,\r\n",
        "                 bm25 = bm25, \r\n",
        "                 corpus = corpus,\r\n",
        "                 n_results = 5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: \"what is anarchism?\"\n",
            "\n",
            "Top 5 results from Wikipedia:\n",
            "\n",
            "\n",
            "1. an author is the creator or originator of any written work such as book or play and is also considered writer more broadly defined an author is the person who originated or gave existence to anything and whose authorship determines responsibility for what was created legal significance of authorship typically the first owner of copyright is the person who created the work the author if more than one person created the work then case of joint authorship can be made provided some criteria are met in the copyright laws of various jurisdictions there is necessity for little flexibility regarding what\n",
            "\n",
            "\n",
            "2. aesthetics or esthetics is branch of philosophy that deals with the nature of beauty and taste as well as the philosophy of art its own area of philosophy that comes out of aesthetics it examines subjective and sensori emotional values or sometimes called judgments of sentiment and taste aesthetics covers both natural and artificial sources of aesthetic experience and judgment it considers what happens in our minds when we engage with aesthetic objects or environments such as in viewing visual art listening to music reading poetry experiencing play exploring nature and so on the philosophy of art specifically studies how\n",
            "\n",
            "\n",
            "3. alternate history alternative history in commonwealth english or simply althist sometimes abbreviated as ah is genre of speculative fiction consisting of stories in which one or more historical events occur differently these stories usually contain what if scenarios at crucial points in history and present outcomes other than those in the historical record the stories are conjectural but are sometimes based on fact alternate history has been seen as subgenre of literary fiction science fiction or historical fiction alternate history works may use tropes from any or all of these genres another term occasionally used for the genre is allohistory\n",
            "\n",
            "\n",
            "4. alfred habdank skarbek korzybski july march was polish american independent scholar who developed field called general semantics which he viewed as both distinct from and more encompassing than the field of semantics he argued that human knowledge of the world is limited both by the human nervous system and the languages humans have developed and thus no one can have direct access to reality given that the most we can know is that which is filtered through the brain responses to reality his best known dictum is the map is not the territory early life and career alfred korzybski family\n",
            "\n",
            "\n",
            "5. ælle also aelle or ella is recorded in early sources as the first king of the south saxons reigning in what is now called sussex england from to perhaps as late as according to the anglo saxon chronicle ælle and three of his sons are said to have landed at place called cymensora and fought against the local britons the chronicle goes on to report victory in at present day pevensey where the battle ended with the saxons slaughtering their opponents to the last man ælle was the first king recorded by the th century chronicler bede to have held\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwRGXW_bWW7G"
      },
      "source": [
        "In the above results, we can see clearly how BM25 fails as the queries become more complicated. This could mainly due to the fact that it is a pure TF-IDF method that does not prioritize keywords in the query over other words. \r\n",
        "\r\n",
        "This problem could be solved by combining BM25 with more advanced text processing techniques. Indeed, as we can see, we are not applying any advanced processing techniques such as lemmatization or keyword extraction. Further experiments will work on implementing these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6eH00gp28XO"
      },
      "source": [
        "### **BERT-Based Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOWPQjR_3Aam"
      },
      "source": [
        "Following Nogueira and Cho's (2019) method, we try to implement BERT as a document re-ranker that will rank the relevance of the documents in the corpus with respect to a given query. \r\n",
        "\r\n",
        "As we know, BERT for classification tasks takes two sentences as input. Given a document $D$ and a query $Q$ that have been tokenized using a BERT tokenizer, we concatenate the query (Sentence 1) and the document (Sentence 2) together, separating them with a `[CLS]` classification token, and feed them to the original pre-trained BERT model implement as a binary classifier where the two classes are: \r\n",
        "\r\n",
        "$$\r\n",
        "\\begin{cases}\r\n",
        "0 = \\text{not relevant}, \\\\\r\n",
        "1 = \\text{relevant}\r\n",
        "\\end{cases}\r\n",
        "$$\r\n",
        "\r\n",
        "As such, BERT will return the probability of document $D$ being relevant to the query $Q$. Given a certain query $Q$, we apply this method on all documents $D_1, D_2, ..., D_n$ in the corpus and get a *relevance score* for each of them. The documents are then ranked by their obtained scores from most relevant to least relevant (similarly to BM25) and this will be the result of our information retrieval task.\r\n",
        "\r\n",
        "First we start by preparing everything for the model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ADn7H39EQe"
      },
      "source": [
        "We retrieve the BERT configurations directory from official Google servers, and read the BERT configs from `json` file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybWkIjUC8bBf",
        "outputId": "ca1002a2-5b23-439e-8d1c-0514d788af04"
      },
      "source": [
        "# Retrieve BERT configs directory from official Google servers\r\n",
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\r\n",
        "\r\n",
        "# Let's take a look at the content of the directory\r\n",
        "tf.io.gfile.listdir(gs_folder_bert)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bert_config.json',\n",
              " 'bert_model.ckpt.data-00000-of-00001',\n",
              " 'bert_model.ckpt.index',\n",
              " 'vocab.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCwx3EQn84MA",
        "outputId": "3dd49e65-4267-4f67-be5a-f8de500dc070"
      },
      "source": [
        "# Read BERT configs\r\n",
        "bert_config_file = os.path.join(gs_folder_bert, 'bert_config.json')\r\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\r\n",
        "bert_config = bert.configs.BertConfig.from_dict(config_dict)\r\n",
        "\r\n",
        "# Take a look at the BERT configs\r\n",
        "config_dict"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_probs_dropout_prob': 0.1,\n",
              " 'hidden_act': 'gelu',\n",
              " 'hidden_dropout_prob': 0.1,\n",
              " 'hidden_size': 768,\n",
              " 'initializer_range': 0.02,\n",
              " 'intermediate_size': 3072,\n",
              " 'max_position_embeddings': 512,\n",
              " 'num_attention_heads': 12,\n",
              " 'num_hidden_layers': 12,\n",
              " 'type_vocab_size': 2,\n",
              " 'vocab_size': 30522}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuJj7Z9P-kk0"
      },
      "source": [
        "Now set up the BERT tokenizer that will be used to tokenize both the Wikipedia articles and the queries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB9T28RY8jCj",
        "outputId": "2da81dc0-1f3b-4e0d-b9a3-a63a6a6231d3"
      },
      "source": [
        "tokenizer = bert.tokenization.FullTokenizer(\r\n",
        "    vocab_file = os.path.join(gs_folder_bert, 'vocab.txt'),\r\n",
        "    do_lower_case = True\r\n",
        ")\r\n",
        "\r\n",
        "print('Vocab size: ', len(tokenizer.vocab))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  30522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEmK2D3c_FdU"
      },
      "source": [
        "Now, we create functions that will tokenize, encode and prepare our text to be fed into BERT for scoring:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ85CUq18jEH"
      },
      "source": [
        "def encode_text(text, tokenizer):\r\n",
        "    \"\"\"\r\n",
        "    Function that takes a text string and a BERT-compatible tokenizer\r\n",
        "    and returns the tokenized text with the '[SEP]' flag appended, \r\n",
        "    after taking a subset of the tokens' list in order to stay under \r\n",
        "    the 512 BERT max sequence length.\r\n",
        "\r\n",
        "    This function is a utility function for the following 'bert_encode' function.\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    @param text: str,\r\n",
        "        A valid string of text to be tokenized\r\n",
        "    @param tokenizer: BERT.tokenization function,\r\n",
        "        A valid BERT-compatible tokenizer\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    @return tokenized text, list\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # Retrieve tokens from tokenizer\r\n",
        "    tokens = list(tokenizer.tokenize(text))\r\n",
        "    # Take only the first 450 elements\r\n",
        "    tokens = tokens[0:450]\r\n",
        "    # Append [SEP]\r\n",
        "    tokens.append('[SEP]')\r\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "\r\n",
        "def bert_encode(corpus, query, tokenizer):\r\n",
        "    \"\"\"\r\n",
        "    Function that takes a corpus, a query and a tokenizer and returns the \r\n",
        "    query and all texts in the corpus concatenated together and separated by\r\n",
        "    [CLS] flag, then tokenized and ready for BERT.\r\n",
        "\r\n",
        "    This function utilizes the previous utility function 'encode_text'.\r\n",
        "@param corpus: list,\r\n",
        "        A valid list of string elements where each element is an article in our\r\n",
        "        corpus. As returned from 'read_corpus' function. As returned from 'read_corpus' function.\r\n",
        "    @param query: string,\r\n",
        "        A valid text string which is the query for which answers need to be \r\n",
        "        retrieved.\r\n",
        "    @param tokenizer: BERT.tokenization function,\r\n",
        "        A valid BERT-compatible tokenizer.\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    @return inputs: dict,\r\n",
        "        A dictionary containg three elements: \r\n",
        "            - input_word_ids: TF.io.tensor, \r\n",
        "                    The tokenized words ids.\r\n",
        "            - input_mask: TF.io.tensor,\r\n",
        "                    Tensor taking values based on whether the element at each \r\n",
        "                    position is a mask (flag) or not.\r\n",
        "            - input_type_ids: TF.io.tensor,\r\n",
        "                    Tensor taking values based on the type of the input element\r\n",
        "                    at each position.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # Compute corpus length.\r\n",
        "    corpus_length = len(corpus)\r\n",
        "\r\n",
        "    # Transform each article in the corpus to a TF ragged constant.\r\n",
        "    tf_corpus = tf.ragged.constant(\r\n",
        "        [encode_text(article, tokenizer) for article in corpus]\r\n",
        "        )\r\n",
        "\r\n",
        "    # Encode the query, then transform it to a TF ragged constant of same \r\n",
        "    # length as the corpus.\r\n",
        "    encoded_query = encode_text(query, tokenizer)\r\n",
        "    tf_query = tf.ragged.constant(\r\n",
        "        [encoded_query for i in range(corpus_length)]\r\n",
        "        )\r\n",
        "    \r\n",
        "    # Create as many [CLS] flags as the number of articles in the corpus.\r\n",
        "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * tf_corpus.shape[0]\r\n",
        "    # Concatenate all elements together \r\n",
        "    input_word_ids = tf.concat([cls, tf_query, tf_corpus], axis = -1)\r\n",
        "\r\n",
        "    # Create masks tensor...\r\n",
        "    input_mask = tf.ones_like(input_word_ids).to_tensor()\r\n",
        "\r\n",
        "    # Create types tensors...\r\n",
        "    type_cls = tf.zeros_like(cls)\r\n",
        "    type_corpus = tf.zeros_like(tf_corpus)\r\n",
        "    type_query = tf.zeros_like(tf_query)\r\n",
        "    # ... and concatenate them together\r\n",
        "    input_type_ids = tf.concat(\r\n",
        "        [type_cls, type_query, type_corpus],\r\n",
        "        axis = -1\r\n",
        "    ).to_tensor()\r\n",
        "\r\n",
        "    # Prepare results dictionary for returning...\r\n",
        "    inputs = {\r\n",
        "        'input_word_ids' : input_word_ids.to_tensor(),\r\n",
        "        'input_mask' : input_mask,\r\n",
        "        'input_type_ids' : input_type_ids\r\n",
        "    }\r\n",
        "\r\n",
        "    # Return...\r\n",
        "    return inputs"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWIqC5_zDNgR"
      },
      "source": [
        "Let's try our function and see if they work properly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU9asPSf8jHs",
        "outputId": "eb327031-64a5-4e54-d048-082853918f6d"
      },
      "source": [
        "text = 'this is a text to test our functions'\r\n",
        "\r\n",
        "# Try...\r\n",
        "encode_text(text, tokenizer)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2023, 2003, 1037, 3793, 2000, 3231, 2256, 4972, 102]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpiNIMdBDzt5"
      },
      "source": [
        "Now let's try to tokenize and encode our full corpus with a given query and take a look at the specifications of the obtained results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H55fwe_k8jJW"
      },
      "source": [
        "query = 'anarchism'\r\n",
        "query_data1 = bert_encode(\r\n",
        "    corpus = corpus,\r\n",
        "    query = query,\r\n",
        "    tokenizer = tokenizer\r\n",
        ")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-6OjTLtFTP_",
        "outputId": "787e1b36-0892-44b7-8655-0a05248f2678"
      },
      "source": [
        "for key, value in query_data1.items():\r\n",
        "  print(f'{key:15s} shape: {value.shape}')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_word_ids  shape: (1000, 456)\n",
            "input_mask      shape: (1000, 456)\n",
            "input_type_ids  shape: (1000, 456)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-PJ8x17GwML"
      },
      "source": [
        "Everything looks working great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXrrywpnH7io"
      },
      "source": [
        "#### **BERT without Finetuning**\r\n",
        "\r\n",
        "As we know, the corpus on which BERT has been trained contains the **full English Wikipedia** (2,500M words) along with the BooksCorpus (800M words).\r\n",
        "\r\n",
        "For this reason, we thought that we do not need to re-train and finetune BERT for our scoring task, since it has already seen the articles found in our corpus. Being trained on document-level corpus and not word-based ones, BERT would be able to idenitfy the connections between our queries and the articles available in the small corpus that we have.\r\n",
        "\r\n",
        "Furthermore, finetuning BERT would require training again on query-answers data sets such as [**MSMARCO**](https://microsoft.github.io/msmarco/) or [**TREC-CAR**](https://trec.nist.gov/pubs/trec26/papers/Overview-CAR.pdf), which were used by Nogueira and Cho (2019) in their implementation. However, due to network constraints (downloading the huge data sets proved not possible) and computational constraints, as well as time constraints (according to Nogueira and Cho, finetuning BERT required more than 30 hours of training), we were unable to finetune it to our specific task. We assumed that it may provide good reasults 'out-of-the-box', however, experimental results have shown otherwise:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9LXdqPAMMp-"
      },
      "source": [
        "Let's create a BERT classifier and prepare it for precdiciton:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxyH24i2MMoO"
      },
      "source": [
        "# BERT configs already imported\n",
        "bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n",
        "    bert_config, num_labels = 2\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9N6r-a0H6lK"
      },
      "source": [
        "As feeding the whole corpus at once to BERT results in RAM overload, we predict labels in batches instead, manually:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28-r0xti_9pf"
      },
      "source": [
        "def bert_score(inputs, bert_classifier):\n",
        "    \"\"\"\n",
        "    Function that takes a dictionary of inputs returned from\n",
        "    'bert_encode' and computes a relevance score for each of the \n",
        "    documents in the corpus.\n",
        "    \n",
        "    This function is a utility function for the function 'bert_search'\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    @param inputs: dict,\n",
        "        A dictionary containg three elements: \n",
        "            - input_word_ids: TF.io.tensor, \n",
        "                    The tokenized words ids.\n",
        "            - input_mask: TF.io.tensor,\n",
        "                    Tensor taking values based on whether the element at each \n",
        "                    position is a mask (flag) or not.\n",
        "            - input_type_ids: TF.io.tensor,\n",
        "                    Tensor taking values based on the type of the input element\n",
        "                    at each position.\n",
        "        As returned from 'bert_encode'\n",
        "    @param bert_classifier: TF.classifier,\n",
        "        A valid TensorFlow BERT classifier object.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize list of results\n",
        "    results = []\n",
        "    # Counter of scored articles\n",
        "    i = 0\n",
        "    \n",
        "    # For each article in the corpus:\n",
        "    while i < len(corpus):\n",
        "        print(f'Scored {i + 5} examples!')\n",
        "        # Create batch of 5 examples\n",
        "        batch = {\n",
        "            'input_type_ids': inputs['input_type_ids'][i:(i+5)],\n",
        "            'input_mask': inputs['input_mask'][i:(i+5)],\n",
        "            'input_word_ids': inputs['input_word_ids'][i:(i+5)]\n",
        "        }\n",
        "        # Compute scores for articles in the batch\n",
        "        result = bert_classifier(batch, training = False)\n",
        "        results.append(result)\n",
        "        i = i + 6\n",
        "        \n",
        "    # Return obtained results\n",
        "    return results\n",
        "        \n",
        "def bert_search(scores, corpus, n_results = 5):\n",
        "    \"\"\"\n",
        "    Function that takes the scores obtained from the function 'bert_score' and\n",
        "    returns the top n_results most relevant articles in the corpus based\n",
        "    on the scores.\n",
        "    \n",
        "    Parameters\n",
        "    ---------- \n",
        "    @param scores: list,\n",
        "        A valid list of scores as returned by the function bert_score.\n",
        "    @param corpus: list,\n",
        "        A valid list of string elements where each element is an article in our\n",
        "        corpus. As returned from 'read_corpus' function.\n",
        "    @param n_results: int,\n",
        "        A valid positive integer specifying the number of search results \n",
        "        required.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    @return relevant_results: list,\n",
        "        List of the n_results most relevant articles from the corpus.\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve the 2nd score returned by BERT, which is the relevance score.\n",
        "    relevance_score = [scores[i][:, 1] for i in range(len(scores) - 1)]\n",
        "    \n",
        "    # Append all the results in one list\n",
        "    relevance_list = []\n",
        "    for i in range(len(relevance_score) - 1):\n",
        "        for j in range(5):\n",
        "            relevance_list.append(float(relevance_score[i][j]))\n",
        "            \n",
        "    # Retrieve the indices of the top n_results most relevant results.\n",
        "    relevance_list = np.asarray(relevance_list, dtype = np.float32)\n",
        "    idx = (-relevance_list).argsort()[:n_results]\n",
        "    \n",
        "    # Retrieve the relevant results from the corpus.\n",
        "    relevant_results = [corpus[i] for i in idx]\n",
        "    \n",
        "    # Done, return!\n",
        "    return relevant_results"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93jcJwJPsZTg"
      },
      "source": [
        "Let's try to look at the search results using our created functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gWk3_sfsFkE",
        "outputId": "62107e3d-bcc8-4a81-bde1-b613719ac115"
      },
      "source": [
        "scores = bert_score(inputs = query_data1,\r\n",
        "                    bert_classifier = bert_classifier)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scored 5 examples!\n",
            "Scored 11 examples!\n",
            "Scored 17 examples!\n",
            "Scored 23 examples!\n",
            "Scored 29 examples!\n",
            "Scored 35 examples!\n",
            "Scored 41 examples!\n",
            "Scored 47 examples!\n",
            "Scored 53 examples!\n",
            "Scored 59 examples!\n",
            "Scored 65 examples!\n",
            "Scored 71 examples!\n",
            "Scored 77 examples!\n",
            "Scored 83 examples!\n",
            "Scored 89 examples!\n",
            "Scored 95 examples!\n",
            "Scored 101 examples!\n",
            "Scored 107 examples!\n",
            "Scored 113 examples!\n",
            "Scored 119 examples!\n",
            "Scored 125 examples!\n",
            "Scored 131 examples!\n",
            "Scored 137 examples!\n",
            "Scored 143 examples!\n",
            "Scored 149 examples!\n",
            "Scored 155 examples!\n",
            "Scored 161 examples!\n",
            "Scored 167 examples!\n",
            "Scored 173 examples!\n",
            "Scored 179 examples!\n",
            "Scored 185 examples!\n",
            "Scored 191 examples!\n",
            "Scored 197 examples!\n",
            "Scored 203 examples!\n",
            "Scored 209 examples!\n",
            "Scored 215 examples!\n",
            "Scored 221 examples!\n",
            "Scored 227 examples!\n",
            "Scored 233 examples!\n",
            "Scored 239 examples!\n",
            "Scored 245 examples!\n",
            "Scored 251 examples!\n",
            "Scored 257 examples!\n",
            "Scored 263 examples!\n",
            "Scored 269 examples!\n",
            "Scored 275 examples!\n",
            "Scored 281 examples!\n",
            "Scored 287 examples!\n",
            "Scored 293 examples!\n",
            "Scored 299 examples!\n",
            "Scored 305 examples!\n",
            "Scored 311 examples!\n",
            "Scored 317 examples!\n",
            "Scored 323 examples!\n",
            "Scored 329 examples!\n",
            "Scored 335 examples!\n",
            "Scored 341 examples!\n",
            "Scored 347 examples!\n",
            "Scored 353 examples!\n",
            "Scored 359 examples!\n",
            "Scored 365 examples!\n",
            "Scored 371 examples!\n",
            "Scored 377 examples!\n",
            "Scored 383 examples!\n",
            "Scored 389 examples!\n",
            "Scored 395 examples!\n",
            "Scored 401 examples!\n",
            "Scored 407 examples!\n",
            "Scored 413 examples!\n",
            "Scored 419 examples!\n",
            "Scored 425 examples!\n",
            "Scored 431 examples!\n",
            "Scored 437 examples!\n",
            "Scored 443 examples!\n",
            "Scored 449 examples!\n",
            "Scored 455 examples!\n",
            "Scored 461 examples!\n",
            "Scored 467 examples!\n",
            "Scored 473 examples!\n",
            "Scored 479 examples!\n",
            "Scored 485 examples!\n",
            "Scored 491 examples!\n",
            "Scored 497 examples!\n",
            "Scored 503 examples!\n",
            "Scored 509 examples!\n",
            "Scored 515 examples!\n",
            "Scored 521 examples!\n",
            "Scored 527 examples!\n",
            "Scored 533 examples!\n",
            "Scored 539 examples!\n",
            "Scored 545 examples!\n",
            "Scored 551 examples!\n",
            "Scored 557 examples!\n",
            "Scored 563 examples!\n",
            "Scored 569 examples!\n",
            "Scored 575 examples!\n",
            "Scored 581 examples!\n",
            "Scored 587 examples!\n",
            "Scored 593 examples!\n",
            "Scored 599 examples!\n",
            "Scored 605 examples!\n",
            "Scored 611 examples!\n",
            "Scored 617 examples!\n",
            "Scored 623 examples!\n",
            "Scored 629 examples!\n",
            "Scored 635 examples!\n",
            "Scored 641 examples!\n",
            "Scored 647 examples!\n",
            "Scored 653 examples!\n",
            "Scored 659 examples!\n",
            "Scored 665 examples!\n",
            "Scored 671 examples!\n",
            "Scored 677 examples!\n",
            "Scored 683 examples!\n",
            "Scored 689 examples!\n",
            "Scored 695 examples!\n",
            "Scored 701 examples!\n",
            "Scored 707 examples!\n",
            "Scored 713 examples!\n",
            "Scored 719 examples!\n",
            "Scored 725 examples!\n",
            "Scored 731 examples!\n",
            "Scored 737 examples!\n",
            "Scored 743 examples!\n",
            "Scored 749 examples!\n",
            "Scored 755 examples!\n",
            "Scored 761 examples!\n",
            "Scored 767 examples!\n",
            "Scored 773 examples!\n",
            "Scored 779 examples!\n",
            "Scored 785 examples!\n",
            "Scored 791 examples!\n",
            "Scored 797 examples!\n",
            "Scored 803 examples!\n",
            "Scored 809 examples!\n",
            "Scored 815 examples!\n",
            "Scored 821 examples!\n",
            "Scored 827 examples!\n",
            "Scored 833 examples!\n",
            "Scored 839 examples!\n",
            "Scored 845 examples!\n",
            "Scored 851 examples!\n",
            "Scored 857 examples!\n",
            "Scored 863 examples!\n",
            "Scored 869 examples!\n",
            "Scored 875 examples!\n",
            "Scored 881 examples!\n",
            "Scored 887 examples!\n",
            "Scored 893 examples!\n",
            "Scored 899 examples!\n",
            "Scored 905 examples!\n",
            "Scored 911 examples!\n",
            "Scored 917 examples!\n",
            "Scored 923 examples!\n",
            "Scored 929 examples!\n",
            "Scored 935 examples!\n",
            "Scored 941 examples!\n",
            "Scored 947 examples!\n",
            "Scored 953 examples!\n",
            "Scored 959 examples!\n",
            "Scored 965 examples!\n",
            "Scored 971 examples!\n",
            "Scored 977 examples!\n",
            "Scored 983 examples!\n",
            "Scored 989 examples!\n",
            "Scored 995 examples!\n",
            "Scored 1001 examples!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG9LsoRUP3dF",
        "outputId": "e7370fc8-3af3-4208-d986-bb8588759c73"
      },
      "source": [
        "n_results = 5\r\n",
        "results = bert_search(scores, corpus, n_results)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([-0.16832183 -0.16766648 -0.17123891 -0.18343538 -0.18021578], shape=(5,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfUY1z4eRGNr",
        "outputId": "036f6a8f-8310-465c-b315-fcae3a39d6f5"
      },
      "source": [
        "print(f'Query: \"{query}\"\\n')\r\n",
        "print(f'Top {n_results} results from Wikipedia:\\n\\n')\r\n",
        "i = 1\r\n",
        "for result in results: \r\n",
        "    print(f'{i}. {result}\\n\\n')\r\n",
        "    i = i + 1"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: \"anarchism\"\n",
            "\n",
            "Top 5 results from Wikipedia:\n",
            "\n",
            "\n",
            "1. apollo was an october space mission carried out by the united states it was the first crewed flight in nasa apollo program and saw the resumption of human spaceflight by the agency after the fire that killed the three apollo astronauts in january the apollo crew was commanded by walter schirra with command module pilot donn eisele and lunar module pilot walter cunningham so designated even though apollo did not carry lunar module the three astronauts were originally designated for the second crewed apollo flight and then as backups for apollo after the fire crewed flights were suspended while the cause of the accident was investigated and improvements made to the spacecraft and safety procedures and uncrewed test flights made determined to prevent repetition of the fire the crew spent long periods of time monitoring the construction of their apollo command and service modules csm training continued over much of the month pause that followed the apollo disaster apollo was launched on october from cape kennedy air force station florida and splashed down in the atlantic ocean eleven days later extensive testing of the csm took place and also the first live television broadcast from an american spacecraft despite tension between the crew and ground controllers the mission was complete technical success giving nasa the confidence to send apollo into orbit around the moon two months later in part because of these tensions none of the crew flew in space again though schirra had already announced he would retire from nasa after the flight apollo fulfilled apollo mission of testing the csm in low earth orbit and was significant step forward towards nasa goal of landing astronauts on the moon background and personnel schirra one of the original mercury seven astronauts graduated from the united states naval academy in he flew mercury atlas in the fifth crewed flight of project mercury the third to reach orbit and was the command pilot for gemini he was year old captain in the navy at the time of apollo eisele graduated from the naval academy in with in aeronautics he elected to be commissioned in the air force and was year old major at the time of apollo cunningham joined the navy in began flight training the following year and served in marine flight squadron from to and was civilian aged serving in the marine corps reserves with rank of major at the time of apollo he received degrees in physics from ucla in and an in both eisele and cunningham were selected as part of the third group of astronauts in schirra crew trains for apollo eisele was originally slotted for position on gus grissom apollo crew along with ed white but days prior to the official announcement on march eisele sustained shoulder injury that would require surgery instead roger chaffee was given the position and eisele was reassigned to schirra crew schirra eisele and cunningham were first named as an apollo crew on september they were to fly second earth orbital test of the apollo command module cm although delighted as rookie to be assigned to prime crew without having served as backup cunningham was troubled by the fact that apollo seemed unnecessary if apollo was successful he learned later that director of flight crew operations deke slayton another of the mercury seven who had been grounded for medical reasons and supervised the astronauts planned with schirra support to command the mission if he gained medical clearance when this was not forthcoming schirra remained in command of the crew and in november apollo was cancelled and schirra crew assigned as backup to grissom on january grissom crew was conducting launch pad test for their planned february mission when fire broke out in the cabin killing all three men complete safety review of the apollo program followed soon after the fire slayton asked schirra eisele and cunningham to fly the first mission after the pause apollo would use the blockii spacecraft designed for the lunar missions as opposed to the block csm used for apollo which was intended only to be used for the early earth orbit missions as it lacked the capability of docking with lunar module the cm and astronauts spacesuits had been extensively redesigned to reduce any chance of repeat of the accident which killed the first crew schirra crew would test the life support propulsion guidance and control systems during this open ended mission meaning it would be extended as it passed each test the duration was limited to days reduced from the original day limit for apollo the backup crew consisted of thomas stafford as commander john young as command module pilot and eugene cernan as lunar module pilot they became the prime crew of apollo ronald evans john jack swigert and edward givens were assigned to the support crew for the mission givens died in car accident on june and william pogue was assigned as his replacement evans was involved in hardware testing at kennedy space center ksc swigert was the launch capsule communicator capcom and worked on the mission operational aspects pogue spent time modifying procedures the support crew also filled in when the primary and backup crews were unavailable capcoms the person in mission control responsible for communicating with the spacecraft then always an astronaut were evans pogue stafford swigert young and cernan flight directors were glynn lunney gene kranz and gerry griffin preparation according to cunningham schirra originally had limited interest in making third spaceflight beginning to focus on his post nasa career flying the first mission after the fire changed things wally schirra was being pictured as the man chosen to rescue the manned space program and that was task worthy of wally interest eisele noted coming on the heels of the fire we knew the fate and future of the entire manned space program not to mention our own skins was riding on the success or failure of apollo given the circumstances of the fire the crew initially had little confidence\n",
            "\n",
            "\n",
            "2. wood metal eutectic low melting point alloy of bismuth lead tin and cadmium individual grains are seen as the flat surfaces of the crystals an alloy is combination of metals or metals combined with one or more other elements for example combining the metallic elements gold and copper produces red gold gold and silver becomes white gold and silver combined with copper produces sterling silver elemental iron combined with non metallic carbon or silicon produces alloys called steel or silicon steel the resulting mixture forms substance with properties that often differ from those of the pure metals such as increased strength or hardness unlike other substances that may contain metallic bases but do not behave as metals such as aluminium oxide sapphire beryllium aluminium silicate emerald or sodium chloride salt an alloy will retain all the properties of metal in the resulting material such as electrical conductivity ductility opaqueness and luster alloys are used in wide variety of applications from the steel alloys used in everything from buildings to automobiles to surgical tools to exotic titanium alloys used in the aerospace industry to beryllium copper alloys for non sparking tools in some cases combination of metals may reduce the overall cost of the material while preserving important properties in other cases the combination of metals imparts synergistic properties to the constituent metal elements such as corrosion resistance or mechanical strength examples of alloys are steel solder brass pewter duralumin bronze and amalgams an alloy may be solid solution of metal elements single phase where all metallic grains crystals are of the same composition or mixture of metallic phases two or more solutions forming microstructure of different crystals within the metal intermetallic compounds are alloys with defined stoichiometry and crystal structure zintl phases are also sometimes considered alloys depending on bond types see van arkel ketelaar triangle for information on classifying bonding in binary compounds alloys are defined by metallic bonding character the alloy constituents are usually measured by mass percentage for practical applications and in atomic fraction for basic science studies alloys are usually classified as substitutional or interstitial alloys depending on the atomic arrangement that forms the alloy they can be further classified as homogeneous consisting of single phase or heterogeneous consisting of two or more phases or intermetallic introduction liquid bronze being poured into molds during casting brass lamp an alloy is mixture of chemical elements which forms an impure substance admixture that retains the characteristics of metal an alloy is distinct from an impure metal in that with an alloy the added elements are well controlled to produce desirable properties while impure metals such as wrought iron are less controlled but are often considered useful alloys are made by mixing two or more elements at least one of which is metal this is usually called the primary metal or the base metal and the name of this metal may also be the name of the alloy the other constituents may or may not be metals but when mixed with the molten base they will be soluble and dissolve into the mixture the mechanical properties of alloys will often be quite different from those of its individual constituents metal that is normally very soft malleable such as aluminium can be altered by alloying it with another soft metal such as copper although both metals are very soft and ductile the resulting aluminium alloy will have much greater strength adding small amount of non metallic carbon to iron trades its great ductility for the greater strength of an alloy called steel due to its very high strength but still substantial toughness and its ability to be greatly altered by heat treatment steel is one of the most useful and common alloys in modern use by adding chromium to steel its resistance to corrosion can be enhanced creating stainless steel while adding silicon will alter its electrical characteristics producing silicon steel like oil and water molten metal may not always mix with another element for example pure iron is almost completely insoluble with copper even when the constituents are soluble each will usually have saturation point beyond which no more of the constituent can be added iron for example can hold maximum of carbon although the elements of an alloy usually must be soluble in the liquid state they may not always be soluble in the solid state if the metals remain soluble when solid the alloy forms solid solution becoming homogeneous structure consisting of identical crystals called phase if as the mixture cools the constituents become insoluble they may separate to form two or more different types of crystals creating heterogeneous microstructure of different phases some with more of one constituent than the other however in other alloys the insoluble elements may not separate until after crystallization occurs if cooled very quickly they first crystallize as homogeneous phase but they are supersaturated with the secondary constituents as time passes the atoms of these supersaturated alloys can separate from the crystal lattice becoming more stable and forming second phase that serves to reinforce the crystals internally some alloys such as electrum an alloy of silver and gold occur naturally meteorites are sometimes made of naturally occurring alloys of iron and nickel but are not native to the earth one of the first alloys made by humans was bronze which is mixture of the metals tin and copper bronze was an extremely useful alloy to the ancients because it is much stronger and harder than either of its components steel was another common alloy however in ancient times it could only be created as an accidental byproduct from the heating of iron ore in fires smelting during the manufacture of iron other ancient alloys include pewter brass and pig iron in the modern age steel can be created in many forms carbon steel can be made by varying only the carbon content producing soft alloys like mild steel or hard alloys like spring steel alloy steels can be made by adding\n",
            "\n",
            "\n",
            "3. casa batlló is building in the center of barcelona it was designed by antoni gaudí and is considered one of his masterpieces remodel of previously built house it was redesigned in by gaudí and has been refurbished several times after that gaudí assistants domènec sugrañes gras josep canaleta and joan rubió also contributed to the renovation project the local name for the building is casa dels ossos house of bones as it has visceral skeletal organic quality it is located on the passeig de gràcia in the eixample district and forms part of row of houses known as the illa de la discòrdia or mansana de la discòrdia the block of discord which consists of four buildings by noted modernista architects of barcelona like everything gaudí designed casa batlló is only identifiable as modernisme or art nouveau in the broadest sense the ground floor in particular has unusual tracery irregular oval windows and flowing sculpted stone work there are few straight lines and much of the façade is decorated with colorful mosaic made of broken ceramic tiles trencadís the roof is arched and was likened to the back of dragon or dinosaur common theory about the building is that the rounded feature to the left of centre terminating at the top in turret and cross represents the lance of saint george patron saint of catalonia gaudí home which has been plunged into the back of the dragon history initial construction antoni gaudí in the building that is now casa batlló was built in commissioned by lluís sala sánchez it was classical building without remarkable characteristics within the eclecticism traditional by the end of the th century the building had basement ground floor four other floors and garden in the back batlló family the batlló family the house was bought by josep batlló in the design of the house made the home undesirable to buyers but the batlló family decided to buy the place due to its centralized location it is located in the middle of passeig de gracia which in the early th century was known as very prestigious and fashionable area it was an area where the prestigious family could draw attention to themselves in josep batlló still owned the home the batlló family was very well known in barcelona for its contribution to the textile industry in the city mr josep batlló casanovas was textile industrialist who owned few factories in the city mr batlló married amalia godo belaunzaran from the family that founded the newspaper la vanguardia josep wanted an architect that would design house that was like no other and stood out as being audacious and creative both josep and his wife were open to anything and they decided not to limit gaudí josep did not want his house to resemble any of the houses of the rest of the batlló family such as casa pía built by the josep vilaseca he chose the architect who had designed park güell because he wanted him to come up with risky plan the family lived on the noble floor of casa batlló until the middle of the renovation the atrium gaudí convinced batlló to let him expand the central well of the building to let in light instead of rebuilding in josep batlló hired gaudí to design his home at first his plans were to tear down the building and construct completely new house gaudí convinced josep that renovation was sufficient and was also able to submit the planning application the same year the building was completed and refurbished in he completely changed the main apartment which became the residence for the batlló family he expanded the central well in order to supply light to the whole building and also added new floors in the same year the barcelona city council selected the house as candidate for that year best building award the award was given to another architect that year despite gaudí design refurbishments josep batlló died in and the house was kept in order by the wife until her death in after the death of the two parents the house was kept and managed by the children until in an insurance company named seguros iberia acquired casa batlló and set up offices there in the first refurbishment occurred mainly in several of the interior rooms of the house in the exterior balconies were restored to their original colour and year later the exterior façade was illuminated in the ceremony of la mercè multiple uses in the current owners of casa batlló bought the home and continued refurbishments throughout the whole building two years later in casa batlló began to hire out its facilities for different events more than square meters of rooms within the building were rented out for many different functions due to the building location and the beauty of the facilities being rented the rooms of casa batlló were in very high demand and hosted many important events for the city design casa batlló fireplace seat overview the local name for the building is casa dels ossos house of bones as it has visceral skeletal organic quality the building looks very remarkable like everything gaudí designed only identifiable as modernisme or art nouveau in the broadest sense the ground floor in particular is rather astonishing with tracery irregular oval windows and flowing sculpted stone work it seems that the goal of the designer was to avoid straight lines completely much of the façade is decorated with mosaic made of broken ceramic tiles trencadís that starts in shades of golden orange moving into greenish blues the roof is arched and was likened to the back of dragon or dinosaur common theory about the building is that the rounded feature to the left of centre terminating at the top in turret and cross represents the lance of saint george patron saint of catalonia gaudí home which has been plunged into the back of the dragon loft the loft originally service area has sixty catenary arches the\n",
            "\n",
            "\n",
            "4. alberta is one of the thirteen provinces and territories of canada with an estimated population of people as of the census it is canada fourth most populous province and the most populous of canada three prairie provinces alberta area is approximately alberta is bordered by the provinces of british columbia to the west and saskatchewan to the east the northwest territories to the north and the state of montana to the south alberta is one of three canadian provinces and territories to border only single state it is also one of only two landlocked provinces in the country alberta capital edmonton is near the geographic centre of the province it is the primary supply and service hub for canada crude oil the athabasca oil sands and other northern resource industries about south of edmonton is calgary the largest city in alberta calgary and edmonton anchor alberta two census metropolitan areas which each have populations exceeding one million while the province has census agglomerations indigenous peoples have inhabited what is now alberta for thousands of years prior to european colonization alberta and saskatchewan were originally districts of the northwest territories but became provinces on september key economic sectors in alberta include energy and clean technology agriculture and petrochemicals the oil industry has been pillar of alberta economy since when substantial oil deposits were discovered at leduc no well alberta current premier is jason kenney of the united conservative party which holds majority in the alberta legislative assembly tourist destinations in the province include banff canmore drumheller jasper sylvan lake and lake louise alberta is home to six unesco world heritage sites the canadian rocky mountain parks dinosaur provincial park the head smashed in buffalo jump waterton glacier international peace park wood buffalo national park and writing on stone áísínai pi the province has predominantly humid continental climate with stark contrasts over year but seasonal temperature average swings are smaller than in areas further east due to winters being warmed by occasional chinook winds bringing sudden warming etymology alberta was named after princess louise caroline alberta the fourth daughter of queen victoria princess louise was the wife of john campbell marquess of lorne governor general of canada lake louise and mount alberta were also named in her honour the name alberta itself is feminine latinized form of the name albert cf masculine albertus in medieval latin and its germanic cognates ultimately derived from proto germanic aþalaberhtaz compound of noble bright famous geography topographic map of alberta showing cities towns municipal district county and rural municipality borders and natural features alberta with an area of is the fourth largest province after quebec ontario and british columbia alberta southern border is the th parallel north which separates it from the state of montana while the th parallel north divides it from the northwest territories the th meridian west separates it from the province of saskatchewan while on the west its boundary with british columbia follows the th meridian west south from the northwest territories at until it reaches the continental divide at the rocky mountains and from that point follows the line of peaks marking the continental divide in generally southeasterly direction until it reaches the montana border at the province extends north to south and east to west at its maximum width its highest point is at the summit of mount columbia in the rocky mountains along the southwest border while its lowest point is on the slave river in wood buffalo national park in the northeast with the exception of the semi arid steppe of the south eastern section the province has adequate water resources there are numerous rivers and lakes used for swimming fishing and range of water sports there are three large lakes lake claire in wood buffalo national park lesser slave lake and lake athabasca which lies in both alberta and saskatchewan the longest river in the province is the athabasca river which travels from the columbia icefield in the rocky mountains to lake athabasca the largest river is the peace river with an average flow of the peace river originates in the rocky mountains of northern british columbia and flows through northern alberta and into the slave river tributary of the mackenzie river alberta capital city edmonton is located at about the geographic centre of the province it is the most northerly major city in canada and serves as gateway and hub for resource development in northern canada the region with its proximity to canada largest oil fields has most of western canada oil refinery capacity calgary is about south of edmonton and north of montana surrounded by extensive ranching country almost of the province population lives in the calgary edmonton corridor the land grant policy to the railroads served as means to populate the province in its early years moraine lake at banff national park the alberta mountain forests makes up the southwestern boundary of alberta most of the northern half of the province is boreal forest while the rocky mountains along the southwestern boundary are largely forested see alberta mountain forests and alberta british columbia foothills forests the southern quarter of the province is prairie ranging from shortgrass prairie in the southeastern corner to mixed grass prairie in an arc to the west and north of it the central aspen parkland region extending in broad arc between the prairies and the forests from calgary north to edmonton and then east to lloydminster contains the most fertile soil in the province and most of the population much of the unforested part of alberta is given over either to grain or to dairy farming with mixed farming more common in the north and centre while ranching and irrigated agriculture predominate in the south the alberta badlands are located in southeastern alberta where the red deer river crosses the flat prairie and farmland and features deep canyons and striking landforms dinosaur provincial park near brooks alberta showcases the badlands terrain desert flora and remnants from alberta past when dinosaurs\n",
            "\n",
            "\n",
            "5. anaximenes of miletus was an ancient greek pre socratic philosopher active in the latter half of the th century bc the details of his life are obscure because none of his work has been preserved anaximenes ideas and philosophies are only known today because of comments made by aristotle and other writers on the history of greek philosophy as one of the three philosophers of the milesian school considered the first revolutionary thinkers of the western world anaximenes is best known and identified as younger friend or student of anaximander who was himself taught by thales each philosopher developed distinct cosmology without completely rejecting their teacher view of the universe or creating major disagreement between them some of anaximenes writings apparently survived the hellenistic age but no record of these documents currently exist much of his astronomical thought was based on anaximander though he altered anaximander astrological ideas to better fit his own philosophical views on physics and the natural world like others in his school of thought anaximenes practiced material monism this tendency to identify one specific underlying reality made up of material thing is what he is principally remembered for today apollodorus of damascus noted the dates anaximander was alive in relation to well known historical events and estimated anaximenes lifespan as having occurred during the same time period in which cyrus the great defeated croesus at the battle of thymbra in bc anaximenes was the last known milesian philosopher as miletus was captured by the persian army in bc anaximenes and the arche while his predecessors thales and anaximander proposed that the archai singular arche meaning the underlying material of the world were water and the ambiguous substance apeiron respectively anaximenes asserted that aer mist vapor air was this primary substance of which all natural things are made by rejecting his teacher theory based on the concept of discontinuity anaximenes took more empirical approach to understanding the underlying processes of genesis and change on two assumptions origination retains properties of the apeiron but it has an actually tangible state of existence as air that can evolve other substances and genesis and change depend on cohesive mechanistic process known as condensation and rarefaction anaximenes believed that air was infinite and divine he was the first to use the word pneuma breath of life as synonym for air one of the only surviving quotes by anaximenes reads just as our soul being air holds us together so pneuma and air encompass and guard the whole world the analogy compared atmospheric air as the divine and human air as souls that animate people this relation of the macroscopic and microscopic suggested anaximenes believed there was an overarching principle that regulated all life and behavior essentially he thought air was the primary substance that held the universe together interestingly the old testament features similar analogy to the founding of the world and creation of man but anaximenes did not recognize creator of the universe and did not think of the pneuma as creator to guide man the choice of air may seem arbitrary but anaximenes based his conclusion on naturally observable phenomena in the processes of rarefaction and condensation the primary difference in the forms of air as matter was the degree of condensation and density when air condenses it becomes visible and according to anaximenes the spread out invisible infinite air was condensed to wind then formed into clouds which condensed further to produce mist rain and other forms of precipitation as the condensed air cooled anaximenes supposed that earth itself was an early condensate of air the process continued until the air was condensed enough to form solids like the earth and ultimately stones by contrast anaximenes was able to visually see how water evaporates into air and based his concept of rarefaction on this observation according to him any object that held light was made of fire and fire was made from the rarefaction of air while other philosophers also recognized such transitions in states of matter anaximenes was the first to associate the qualitative change in hot dry and cold wet pairings with the density of single material effectively adding quantitative dimension to the milesian monistic system he attributed condensation to cold wet air and rarefaction to the interaction of hot dry air this concept was the foundation for understanding the existence of different substances materials and elements due to their arrangement of atoms and number of subatomic particles influence on philosophy since language and communication were very limited in his time anaximenes analogies were key in explaining the uncertain through the certain for example he knew for certain that blowing air on his hand with his mouth wide open produced hot air while blowing on his hand with half closed lips produced cold air these observations were key in his postulate that the hot air was due to rarefaction and expansion whereas the cold air was due to condensation and compression although in modern times it is known that this is actually the opposite anaximenes was key in arriving at this conclusion his analogies often connected parallels between man and the cosmos insinuating that the same natural laws observable on earth applied to the heavens over years later isaac newton proved this to be true throughout history anaximenes observations proved helpful to uncover powerful theories such as quantum physics and chemical properties by the end of the milesian philosophy era there were many questions left unanswered this sparked the stimulation of pre socratic thought to continue through many other notable philosophers such as pythagoras parmenides heraclitus and democritus anaximenes greatest influence is not from his theories of matter but instead it is from how he thought about these ideas for instance his theory of air being the underlying substance was disproved but when looking at his idea from fundamental aspect in which substance is capable of changing forms his theory was the first of its kind this concept of changing of forms is fundamental to scientific\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WArwzGx_9pg"
      },
      "source": [
        "Unfortunately, we notice that BERT does not provide results that are relevant to the query. However, we believe that this downside can be solved if finetuning to our task was possible.\n",
        "\n",
        "In reality, the main downside we find to BERT, in comparison to BM25, is time consumption. Indeed, ranking all documents in our corpus using BM25 is a task that takes only a couple of seconds using BM25 but consumes time in the order to minutes (more than 10 minutes in general) using BERT. This can be a game-changing downside especially since most users in information retrieval applications are not only looking for reliable and correct results, but also *fast* results. That is, that can be retrieved quickly.\n",
        "\n",
        "**Note**: In order to use BERT trained on MS-MARCO, simply obtain the model from the official Nogueira and Cho (2019) repository (link provided in README) then load its `configs` as done previously, instead of the original BERT configuations.\n",
        "\n",
        "---"
      ]
    }
  ]
}