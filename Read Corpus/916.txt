the theoretical speedup of the latency of the execution of program as function of the number of processors executing it according to amdahl law the speedup is limited by the serial part of the program for example if of the program can be parallelized the theoretical maximum speedup using parallel computing would be times in computer architecture amdahl law or amdahl argument is formula which gives the theoretical speedup in latency of the execution of task at fixed workload that can be expected of system whose resources are improved it is named after computer scientist gene amdahl and was presented at the afips spring joint computer conference in amdahl law is often used in parallel computing to predict the theoretical speedup when using multiple processors for example if program needs hours to complete using single thread but one hour portion of the program cannot be parallelized therefore only the remaining hours of execution time can be parallelized then regardless of how many threads are devoted to parallelized execution of this program the minimum execution time cannot be less than one hour hence the theoretical speedup is limited to at most times the single thread performance definition amdahl law can be formulated in the following way where latency is the theoretical speedup of the execution of the whole task is the number of threads across which the parallel portion is split is the proportion of execution time that the part benefiting from improved resources originally occupied furthermore shows that the theoretical speedup of the execution of the whole task increases with the improvement of the resources of the system and that regardless of the magnitude of the improvement the theoretical speedup is always limited by the part of the task that cannot benefit from the improvement amdahl law applies only to the cases where the problem size is fixed in practice as more computing resources become available they tend to get used on larger problems larger datasets and the time spent in the parallelizable part often grows much faster than the inherently serial work in this case gustafson law gives less pessimistic and more realistic assessment of the parallel performance derivation task executed by system whose resources are improved compared to an initial similar system can be split up into two parts part that does not benefit from the improvement of the resources of the system part that benefits from the improvement of the resources of the system an example is computer program that processes files from disk part of that program may scan the directory of the disk and create list of files internally in memory after that another part of the program passes each file to separate thread for processing the part that scans the directory and creates the file list cannot be sped up on parallel computer but the part that processes the files can the execution time of the whole task before the improvement of the resources of the system is denoted as it includes the execution time of the part that would not benefit from the improvement of the resources and the execution time of the one that would benefit from it the fraction of the execution time of the task that would benefit from the improvement of the resources is denoted by the one concerning the part that would not benefit from it is therefore then it is the execution of the part that benefits from the improvement of the resources that is accelerated by the factor after the improvement of the resources consequently the execution time of the part that does not benefit from it remains the same while the part that benefits from it becomes the theoretical execution time of the whole task after the improvement of the resources is then amdahl law gives the theoretical speedup in latency of the execution of the whole task at fixed workload which yields parallel programs if of the execution time may be the subject of speedup will be if the improvement makes the affected part twice as fast will be amdahl law states that the overall speedup of applying the improvement will be for example assume that we are given serial task which is split into four consecutive parts whose percentages of execution time are and respectively then we are told that the st part is not sped up so while the nd part is sped up times so the rd part is sped up times so and the th part is sped up times so by using amdahl law the overall speedup is notice how the times and times speedup on the nd and rd parts respectively don have much effect on the overall speedup when the th part of the execution time is accelerated by only times serial programs assume that task has two independent parts and part takes roughly of the time of the whole computation by working very hard one may be able to make this part times faster but this reduces the time of the whole computation only slightly in contrast one may need to perform less work to make part perform twice as fast this will make the computation much faster than by optimizing part even though part speedup is greater in terms of the ratio times versus times for example with serial program in two parts and for which and if part is made to run times faster that is and then if part is made to run times faster that is and then therefore making part to run times faster is better than making part to run times faster the percentage improvement in speed can be calculated as improving part by factor of will increase overall program speed by factor of which makes it faster than the original computation however improving part by factor of which presumably requires more effort will achieve an overall speedup factor of only which makes it faster optimizing the sequential part of parallel programs if the non parallelizable part is optimized by