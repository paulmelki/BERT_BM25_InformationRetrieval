algorithms for calculating variance play major role in computational statistics key difficulty in the design of good algorithms for this problem is that formulas for the variance may involve sums of squares which can lead to numerical instability as well as to arithmetic overflow when dealing with large values naïve algorithm formula for calculating the variance of an entire population of size is using bessel correction to calculate an unbiased estimate of the population variance from finite sample of observations the formula is therefore naïve algorithm to calculate the estimated variance is given by the following let for each datum this algorithm can easily be adapted to compute the variance of finite population simply divide by instead of on the last line because and can be very similar numbers cancellation can lead to the precision of the result to be much less than the inherent precision of the floating point arithmetic used to perform the computation thus this algorithm should not be used in practice and several alternate numerically stable algorithms have been proposed this is particularly bad if the standard deviation is small relative to the mean however the algorithm can be improved by adopting the method of the assumed mean computing shifted data the variance is invariant with respect to changes in location parameter property which can be used to avoid the catastrophic cancellation in this formula with any constant which leads to the new formula the closer is to the mean value the more accurate the result will be but just choosing value inside the samples range will guarantee the desired stability if the values are small then there are no problems with the sum of its squares on the contrary if they are large it necessarily means that the variance is large as well in any case the second term in the formula is always smaller than the first one therefore no cancellation may occur if just the first sample is taken as the algorithm can be written in python programming language as def data if len data this formula also facilitates the incremental computation that can be expressed as ex ex def add_variable global ex ex if ex ex def remove_variable global ex ex ex ex def get_mean global ex return ex def get_variance global ex ex return ex ex ex two pass algorithm an alternative approach using different formula for the variance first computes the sample mean and then computes the sum of the squares of the differences from the mean where is the standard deviation this is given by the following code def data sum sum for in data sum mean sum for in data sum mean mean variance sum return variance this algorithm is numerically stable if is small however the results of both of these simple algorithms naïve and two pass can depend inordinately on the ordering of the data and can give poor results for very large data sets due to repeated roundoff error in the accumulation of the sums techniques such as compensated summation can be used to combat this error to degree welford online algorithm it is often useful to be able to compute the variance in single pass inspecting each value only once for example when the data are being collected without enough storage to keep all the values or when costs of memory access dominate those of computation for such an online algorithm recurrence relation is required between quantities from which the required statistics can be calculated in numerically stable fashion the following formulas can be used to update the mean and estimated variance of the sequence for an additional element here denotes the sample mean of the first samples xn their sample variance and their population variance these formulas suffer from numerical instability as they repeatedly subtract small number from big number which scales with better quantity for updating is the sum of squares of differences from the current mean here denoted this algorithm was found by welford and it has been thoroughly analyzed it is also common to denote and an example python implementation for welford algorithm is given below for new value newvalue compute the new count new mean the new mean accumulates the mean of the entire dataset aggregates the squared distance from the mean count aggregates the number of samples seen so far def update newvalue count mean count delta newvalue mean mean delta count delta newvalue mean delta delta return count mean retrieve the mean variance and sample variance from an aggregate def finalize count mean if count this algorithm is much less prone to loss of precision due to catastrophic cancellation but might not be as efficient because of the division operation inside the loop for particularly robust two pass algorithm for computing the variance one can first compute and subtract an estimate of the mean and then use this algorithm on the residuals the parallel algorithm below illustrates how to merge multiple sets of statistics calculated online weighted incremental algorithm the algorithm can be extended to handle unequal sample weights replacing the simple counter with the sum of weights seen so far west suggests this incremental algorithm def w_sum w_sum mean for in alternatively for in zip data weights w_sum w_sum w_sum w_sum mean_old mean mean mean_old w_sum mean_old mean_old mean w_sum bessel correction for weighted samples frequency weights w_sum reliability weights w_sum w_sum w_sum parallel algorithm chan et al note that welford online algorithm detailed above is special case of an algorithm that works for combining arbitrary sets and this may be useful when for example multiple processing units may be assigned to discrete parts of the input chan method for estimating the mean is numerically unstable when and both are large because the numerical error in is not scaled down in the way that it is in the case in such cases prefer def n_a avg_a n_b avg_b n_a n_b delta avg_b avg_a delta n_a n_b var_ab return var_ab this can be generalized to allow parallelization