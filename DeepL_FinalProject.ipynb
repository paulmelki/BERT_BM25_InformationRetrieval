{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Toulouse School of Economics**\n",
    "#### **M2 Statistics & Econometrics**\n",
    "---\n",
    "\n",
    "### **Mathematics of Deep Learning Algorithms, Part 2**\n",
    "# **Final Project: *Performance Benchmarking of Different Information Retrieval Methods***\n",
    "\n",
    "### **Anh-Dung LE, Paul MELKI**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we aim at comparing the performance of different Information Retrieval techniques, mainly: **BM25** and **BERT-based search engine**. We work on a corpus formed of the latest dump of English Wikipedia, and restrict our work to only a small subset of this dump (mainly, articles whose title starts with the letter 'A'), and that is due to unavailability of enough computational resources. \n",
    "\n",
    "But first, we start with some preliminary steps: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preliminaries & Corpus Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from gensim.corpora import WikiCorpus\n",
    "from rank_bm25 import BM25Okapi, BM25Plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create our own local textual corpus based from Wikipedia, we make use of the class `WikiCorpus` implement in the `gensim.corpora` library. This class implements different functions that facilitate the handling and manipulation of Wikipedia dumps, which are usually downloaded as BZ2-compressed XML files.\n",
    "\n",
    "Based on this library, we create our own function to read and save the corpus locally, with each Wikipedia being saved in its own `.txt` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to read and create corpus from downloaded dump\n",
    "def make_corpus(in_file, out_directory):\n",
    "    \"\"\"\n",
    "    Function that converts a Wikipedia .xml dump into a \n",
    "    corpus, saving each article in a separate .txt file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    @param in_file: str, \n",
    "        A valid string specifying the path to the local *.xml.bz2 Wikipedia \n",
    "        dump file.\n",
    "    @param out_directory, str,\n",
    "        A valid string specifying the path to the directory in which we wish to\n",
    "        save the created .txt files.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate WikiCorpus object, based on the local dump file.\n",
    "    wiki = WikiCorpus(in_file)\n",
    "    print(\"Corpus is read!\")\n",
    "    \n",
    "    # Initialize counter of articles read.\n",
    "    i = 0\n",
    "    \n",
    "    print(\"Getting texts...\")\n",
    "    # For new article read, do...\n",
    "    for text in wiki.get_texts():\n",
    "        # Create and open new file for new article.\n",
    "        output_file = open(f'{out_directory}\\\\{str(i+1)}.txt', 'w')\n",
    "        # Extract the text of the read article.\n",
    "        article_text = bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n'\n",
    "        # Take only first 1000 words from each article, to keep sizes small.\n",
    "        first_n_words = ' '.join(article_text.split(' ')[0:1000])\n",
    "        # Write text to file & close the file.\n",
    "        output_file.write(first_n_words)\n",
    "        output_file.close()\n",
    "        # Update counter\n",
    "        i = i + 1\n",
    "        # If 1000 articles have been read, stop reading.\n",
    "        if (i % 1000 == 0):\n",
    "            print(f'Processed {str(i)} articles')\n",
    "            break\n",
    "\n",
    "    print('Processing Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-24fb57d3dadc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create corpus!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmake_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'make_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize input and output paths\n",
    "in_path = \"C:\\\\Users\\\\Paul\\\\Documents\\\\Python Scripts\\\\Data\\\\enwiki-latest-pages-articles1.xml-p1p41242.bz2\"\n",
    "out_path = \"C:\\\\Users\\\\Paul\\\\Documents\\\\Python Scripts\\\\Data\\\\Wiki Corpus\"\n",
    "\n",
    "# Create corpus!\n",
    "make_corpus(in_path, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the corpus is created, we also need to create a function to read the corpus from the files we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_directory):\n",
    "    \"\"\"\n",
    "    Function that iteratively reads the saved articles from the corpus directory\n",
    "    and appends the text to a list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    @param corpus_directory: str,\n",
    "        A valid string specifying the path to the local directory in which the \n",
    "        files were saved using make_corpus().\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    @return corpus, list\n",
    "        A list containing the text of an article in each element.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize empty corpus list\n",
    "    corpus = []\n",
    "    \n",
    "    # For each file in the corpus directory, do...\n",
    "    print(\"Reading local corpus, please wait...\")\n",
    "    for filename in os.listdir(corpus_directory):\n",
    "        file = open(f'{corpus_directory}\\\\{filename}', 'r')\n",
    "        article_text = file.read()\n",
    "        corpus.append(article_text)\n",
    "        \n",
    "    # Done, return\n",
    "    print(\"Done!\")\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading local corpus, please wait...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'atm or atm often refers to atmosphere unit or atm unit of atmospheric pressure automated teller mach'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read corpus! \n",
    "corpus = read_corpus(\"C:\\\\Users\\\\Paul\\\\Documents\\\\Python Scripts\\\\Data\\\\Wiki Corpus\")\n",
    "# Look at some example...\n",
    "corpus[3][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BM25 Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first Information Retrieval method we try is the **BM25** method, which is a TF-IDF method, that retrieves the article that has the highest score based on the query given. \n",
    "\n",
    "Given, a document $D$ and a $Q$ that contains keywords $q_1,..., q_n$, we define the BM25 score of the document $D$ as:\n",
    "\n",
    "$$\n",
    "score(D, Q) = \\sum_{i = 1}^n IDF(q_i) \\cdot \\frac{TF(q_i, D) \\cdot (k_1 + 1)}{TF(q_i, D) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{|D|}{avgdl} \\right)}\n",
    "$$\n",
    "\n",
    "where: \n",
    "- $TF(q_i, D)$ is the *text frequency* of keyword $q_i$ in document $D$,\n",
    "- $IDF(q_i)$ is the *inverse document frequency* of keyword $q_i$, using the well-known definition,\n",
    "- $|D|$ is the length of the document $D$ in words.\n",
    "- $avgdl$ is the average document length in words in the whole corpus.\n",
    "- $k_1$ and $b$ are free parameters that are chosen rather than estimated, and which are usually chosen as $k_1 \\in [1.2, 2.0]$ and $b = 0.75$. These may also be chosen based on some advanced optimization.\n",
    "\n",
    "After computing the BM25 score of each document, which gives the relevance of each document to the given query, we sort the documents in descending order from most relevant to least relevant.\n",
    "\n",
    "On the implementation side, we use `Rank-BM25` library developed by Dorian Brown (https://github.com/dorianbrown/rank_bm25), and which implements different variants of the BM25 algorithm. It can be easily installed using `pip install rank-bm25`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "# Instantiate BM25 object from the tokenized corpus\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"math\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "bm25.get_top_n(tokenized_query, corpus, n=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
